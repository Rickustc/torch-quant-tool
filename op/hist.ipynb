{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "fp32_hist1 = torch.tensor([-6.0, -2.4, 1.3, 2.3, 3.9, 5.0])\n",
    "fp32_hist1_abs = fp32_hist1.abs()\n",
    "fp32_hist1_abs_max = fp32_hist1_abs.max().item()\n",
    "hist1 = torch.histc(fp32_hist1_abs, 3, 0, fp32_hist1_abs_max)\n",
    "\n",
    "hist1_width = fp32_hist1_abs_max / hist1.numel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fp32_hist2 = torch.tensor([-5.9, -2.4, 2.1, 2.3, 3.2, 3.9, 5.0, 8.0])\n",
    "fp32_hist2_abs = fp32_hist2.abs()\n",
    "fp32_hist2_abs_max = fp32_hist2_abs.max().item()\n",
    "input_absmax = max(fp32_hist2_abs_max, fp32_hist1_abs_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_batches = 20\n",
    "\n",
    "# QAT finetune\n",
    "for nepoch in range(8):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.ao.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
