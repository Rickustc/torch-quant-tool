{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet v2 Quantization with ONNX Runtime on CPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will load a mobilenet v2 model pretrained with [PyTorch](https://pytorch.org/), export the model to ONNX, and quantize then run with ONNXRuntime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "\n",
    "If you have Jupyter Notebook, you can run this notebook directly with it. You may need to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/), and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, install [Anaconda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.8\n",
    "conda activate cpu_env\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install packages\n",
    "Let's install the necessary packages to start the tutorial. We will install PyTorch 1.8, OnnxRuntime 1.8, latest ONNX and pillow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load a pretrained mobilenet v2 model, and export it to ONNX."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the pretrained model\n",
    "Use torchvision provides API to load mobilenet_v2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/quantize/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms as T\n",
    "import timm\n",
    "model = timm.create_model('resnet50', pretrained=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Export the model to ONNX\n",
    "Pytorch onnx export API to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "# # image_height = 256\n",
    "# # image_width = 256\n",
    "# # image_height = 800\n",
    "# # image_width = 1199\n",
    "# image_height = 640\n",
    "# image_width = 640\n",
    "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,              # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"resnet50_float.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']) # the model's output names\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sample Execution with ONNXRuntime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an sample with the full precision ONNX model. Firstly, implement the preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "def preprocess_image(image_path, height, width, channels=3):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((width, height), Image.ANTIALIAS)\n",
    "    image_data = np.asarray(image).astype(np.float32)\n",
    "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
    "    mean = np.array([0.079, 0.05, 0]) + 0.406\n",
    "    std = np.array([0.005, 0, 0.001]) + 0.224\n",
    "    # print(image_data.shape[0])\n",
    "    for channel in range(image_data.shape[0]):\n",
    "        image_data[channel, :, :] = (image_data[channel, :, :] / 255 - mean[channel]) / std[channel]\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    return image_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the imagenet labels and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "\n",
    "# Read the categories\n",
    "with open(\"/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the example with ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00364494e+00 -2.68677783e+00 -1.24730301e+00 -1.02071035e+00\n",
      "  -1.62582362e+00  2.60259628e-01 -2.42855930e+00 -2.56515920e-01\n",
      "   5.46876013e-01 -1.44562960e+00  1.21661448e+00 -2.51657796e+00\n",
      "   2.63004780e+00 -6.94954753e-01 -2.36902165e+00  1.43998456e+00\n",
      "  -1.27346885e+00 -2.16607976e+00 -4.55389023e+00 -3.44421887e+00\n",
      "  -4.66023125e-02  2.68369770e+00  1.03526270e+00 -1.15526676e+00\n",
      "   2.54656649e+00 -3.57944584e+00 -5.62320828e-01 -2.81269717e+00\n",
      "  -2.70262790e+00 -3.09027582e-01  1.36424506e+00 -3.05889487e+00\n",
      "  -1.36863005e+00 -2.33973145e+00  9.87966120e-01  6.95207179e-01\n",
      "   1.89699376e+00 -3.19865674e-01  2.82360649e+00  3.63455391e+00\n",
      "  -1.81797051e+00  3.94215178e+00  6.08824313e-01  2.95863700e+00\n",
      "   1.33709764e+00  1.54550886e-03 -1.57516509e-01  1.91143107e+00\n",
      "   8.45279276e-01 -1.24554396e+00 -1.37848234e+00  2.57497573e+00\n",
      "  -7.25240707e-01 -3.70089483e+00 -7.25843832e-02 -2.88598824e+00\n",
      "  -3.29884434e+00 -3.49452198e-01 -1.58990216e+00  2.95361608e-01\n",
      "  -5.18140376e-01 -2.09311843e+00 -4.11099419e-02  1.11870968e+00\n",
      "   7.89028406e-01 -2.49272752e+00  4.25653601e+00  9.78952944e-01\n",
      "   2.61018109e+00  5.65183878e-01 -3.09710479e+00 -2.61973739e+00\n",
      "  -2.99980313e-01  1.69870472e+00 -1.24957418e+00 -2.60438061e+00\n",
      "  -1.20842671e+00 -1.79304004e+00  2.83815384e-01 -3.56108189e+00\n",
      "   6.84156895e-01 -2.60298467e+00  3.50563717e+00  3.69759178e+00\n",
      "  -1.06944752e+00 -1.09858978e+00  2.21907282e+00  8.36451530e-01\n",
      "   4.05006677e-01 -3.22270179e+00 -2.98831630e+00 -2.17058086e+00\n",
      "  -1.37204373e+00 -2.44293523e+00 -2.11577463e+00 -1.91558659e+00\n",
      "  -3.23917556e+00 -8.58641624e-01 -3.16709161e-01 -6.98798180e-01\n",
      "  -3.21091509e+00 -1.51443028e+00 -1.10898900e+00 -4.19086695e-01\n",
      "  -2.15544605e+00 -2.62014532e+00  3.50582153e-01 -5.47892141e+00\n",
      "  -4.37235260e+00 -1.16797721e+00 -2.04223680e+00 -1.46936941e+00\n",
      "   1.76090240e+00 -2.49605387e-01 -2.83216619e+00 -3.12233973e+00\n",
      "  -2.54329157e+00  3.67716503e+00 -2.52866626e-01 -2.44223166e+00\n",
      "  -4.08384562e+00 -7.33014941e-01 -7.51523376e-01 -2.47403932e+00\n",
      "  -1.66923702e+00 -1.11039007e+00 -1.55746031e+00 -4.49105406e+00\n",
      "  -2.00606585e+00 -4.16161489e+00 -3.14789653e+00 -2.42908311e+00\n",
      "  -1.55782461e+00  2.49916530e+00 -3.34100795e+00 -2.00297165e+00\n",
      "  -3.41439843e+00 -3.21143413e+00 -1.30569243e+00 -1.03245091e+00\n",
      "  -9.47993279e-01 -6.21070564e-01 -2.15415764e+00 -2.14657426e+00\n",
      "  -2.06526041e+00 -1.68014967e+00 -1.57356179e+00 -5.24045825e-01\n",
      "   7.71451890e-01 -1.75436866e+00  6.08480263e+00  9.77939308e-01\n",
      "  -7.71644562e-02 -1.48977101e+00 -3.12966555e-01  9.21254516e-01\n",
      "  -2.88711429e-01 -1.79476917e+00  4.33549434e-01  7.11917937e-01\n",
      "  -1.88456953e+00 -6.27643108e-01 -3.98517340e-01  1.70383620e+00\n",
      "   8.77922177e-01  5.89911267e-02 -2.37941670e+00 -2.31626678e+00\n",
      "   1.84229016e+00 -3.82784438e+00 -1.52924526e+00 -8.96566987e-01\n",
      "   1.68274924e-01  1.06751883e+00 -9.35886741e-01  7.83239678e-02\n",
      "  -8.96214306e-01 -2.11868286e+00  2.22142363e+00 -5.39308429e-01\n",
      "   7.73218393e-01 -1.87486589e+00 -7.05191851e-01 -5.54291964e-01\n",
      "  -1.50899434e+00  5.44606388e-01  1.92181349e+00  2.67249084e+00\n",
      "  -1.20364869e+00 -1.51596591e-01 -4.68892511e-03 -7.67576471e-02\n",
      "  -1.42393434e+00  2.29438353e+00 -4.54981834e-01  1.16560113e+00\n",
      "   5.48831820e-01  5.14479160e-01  2.92795897e+00 -1.53834611e-01\n",
      "   1.08835697e+00  1.65795100e+00 -2.25216240e-01 -4.87168878e-01\n",
      "   6.80662513e-01 -1.21987200e+00 -8.93132508e-01 -8.57343078e-01\n",
      "   6.82803988e-01  5.41749895e-01  3.48024464e+00  3.08495593e+00\n",
      "  -7.17317700e-01 -2.78851295e+00 -3.48303348e-01  1.19772159e-01\n",
      "   9.69383597e-01 -1.34086561e+00 -1.16872966e+00  2.83502400e-01\n",
      "   1.02609289e+00 -1.59149516e+00 -3.19995308e+00 -7.54075766e-01\n",
      "  -1.80237746e+00 -2.49292541e+00  6.47443056e-01  5.95564604e-01\n",
      "  -3.59006733e-01 -1.73281181e+00 -3.33481383e+00 -2.03031850e+00\n",
      "  -1.51634657e+00  2.86408484e-01 -4.29407001e-01  4.78937954e-01\n",
      "   1.07229590e+00 -2.74025619e-01 -2.00386000e+00 -2.76350546e+00\n",
      "  -1.58623612e+00 -2.55137801e+00  7.20506847e-01  2.40764308e+00\n",
      "  -2.84331942e+00  1.84914398e+00  1.36339080e+00 -2.72036386e+00\n",
      "   5.75043499e-01 -6.47347510e-01 -1.54895395e-01  1.99464057e-02\n",
      "   2.45985612e-01  1.27952993e+00  2.92213321e-01 -1.97702396e+00\n",
      "  -3.23323035e+00 -3.61337876e+00 -3.05462241e+00 -2.35476303e+00\n",
      "  -3.80119896e+00 -2.74512839e+00  7.30930865e-01 -2.01492047e+00\n",
      "  -8.02857518e-01 -2.23017025e+00 -2.73999381e+00 -3.33508706e+00\n",
      "   2.00008368e+00 -8.83009017e-01 -3.27232146e+00 -1.32802284e+00\n",
      "   3.60900670e-01 -6.10048890e-01  4.23795164e-01 -2.46629810e+00\n",
      "  -1.99275684e+00  9.19256389e-01  1.11193085e+00 -2.06332374e+00\n",
      "   2.60414982e+00  1.51434002e+01  1.32467937e+01  6.58378506e+00\n",
      "   4.09445429e+00  1.38630705e+01  4.86391830e+00  9.44917202e+00\n",
      "   6.52362967e+00  4.03333330e+00  5.38989353e+00  3.44441676e+00\n",
      "   9.19458008e+00  7.34941661e-01 -8.64356160e-01 -2.33338618e+00\n",
      "  -3.27462411e+00 -1.74081612e+00  1.47058594e+00  4.41994518e-01\n",
      "  -3.03345609e+00 -5.08723974e+00 -2.07120800e+00 -2.04929566e+00\n",
      "  -4.93812180e+00 -4.18675041e+00 -1.42386246e+00 -2.94117904e+00\n",
      "  -9.25065815e-01 -2.36231089e+00 -4.03786755e+00 -2.34106913e-01\n",
      "  -1.66055942e+00 -3.37815070e+00 -3.40354729e+00 -2.63815379e+00\n",
      "  -1.00980866e+00 -2.80402207e+00 -3.99275351e+00 -1.70144832e+00\n",
      "  -3.60744643e+00 -5.34996367e+00  7.21272588e-01 -2.45106030e+00\n",
      "  -2.94329119e+00 -3.54914045e+00 -2.49918342e-01 -1.15819311e+00\n",
      "  -3.31583476e+00 -3.70402408e+00  3.01232958e+00  3.38509417e+00\n",
      "   1.24690950e+00 -1.02064157e+00 -1.51935089e+00  3.83534956e+00\n",
      "  -2.51899838e+00  7.84913301e-01 -9.58443940e-01 -1.79133832e+00\n",
      "   3.87101841e+00  1.10973215e+00  8.43661785e-01 -1.18743408e+00\n",
      "   1.79822230e+00 -9.41359550e-02 -2.22143793e+00 -4.23356533e+00\n",
      "  -3.60288191e+00 -1.65870559e+00  1.48345232e+00  7.04207793e-02\n",
      "  -1.76219845e+00 -1.85012984e+00 -1.09114075e+00 -3.64423323e+00\n",
      "   1.91475952e+00  2.88087654e+00  2.68063992e-01  1.90065575e+00\n",
      "   3.24348021e+00 -7.02356547e-02 -1.26856267e+00  1.56142187e+00\n",
      "  -5.21014035e-01  6.13163650e-01 -1.02783108e+00  9.97199714e-01\n",
      "  -3.09125924e+00 -2.40141943e-01  6.02095246e-01  1.24134612e+00\n",
      "  -2.26778698e+00  1.53168595e+00  4.56391796e-02 -4.63054895e+00\n",
      "  -4.96981591e-01  1.60094059e+00 -1.85856056e+00 -3.09210348e+00\n",
      "  -9.66310203e-02 -3.34249878e+00 -2.12304640e+00 -1.76661110e+00\n",
      "  -1.91626310e+00  1.28370011e+00 -9.40929353e-01 -2.68666267e+00\n",
      "  -4.70315409e+00  2.19915915e+00 -2.13372135e+00  2.61538923e-01\n",
      "  -1.04383882e-02 -4.61945486e+00 -6.09784245e-01  1.96917164e+00\n",
      "  -4.47221327e+00 -2.08657360e+00  1.94953561e+00  1.17444682e+00\n",
      "  -1.14413977e+00 -5.08037150e-01  1.10005260e+00 -4.27755356e+00\n",
      "  -4.30024099e+00 -1.66899490e+00  2.34757471e+00 -3.49698067e+00\n",
      "  -1.47795618e+00  1.35356867e+00 -4.03633922e-01  4.81773950e-02\n",
      "   2.35553908e+00  1.49013370e-01  4.50752765e-01  1.42150784e+00\n",
      "   3.42692804e+00 -4.93063539e-01 -7.98867345e-01  1.94720912e+00\n",
      "  -6.51252091e-01  1.13101780e+00  8.04337740e-01 -4.52890038e-01\n",
      "   3.00013304e-01 -2.34237599e+00  1.90693569e+00  3.44703150e+00\n",
      "   2.47365761e+00  1.22193861e+00  1.32791924e+00  2.58154774e+00\n",
      "  -2.37874675e+00  5.23669839e-01  3.36946177e+00  4.67267466e+00\n",
      "  -1.59139931e+00 -9.33343649e-01  1.95079648e+00  2.50369525e+00\n",
      "  -2.50079811e-01 -1.20065235e-01  5.20814717e-01  5.63659847e-01\n",
      "  -1.71956325e+00  1.50071716e+00  3.31576228e-01  8.32047343e-01\n",
      "  -8.26029301e-01 -1.34340477e+00 -2.29161906e+00  1.23387647e+00\n",
      "   1.78214681e+00  5.09877729e+00  3.76911491e-01 -2.67548740e-01\n",
      "   3.11592150e+00  4.47853136e+00  4.42192286e-01  2.99629998e+00\n",
      "   1.22787878e-01  1.13994014e+00  3.37065196e+00  4.91463995e+00\n",
      "   1.41918254e+00  6.73925132e-03 -4.27226543e+00  1.92591417e+00\n",
      "  -1.28516650e+00  3.66142511e+00  3.61898756e+00 -9.97500837e-01\n",
      "   1.11000109e+00  2.60279566e-01 -5.73663235e-01 -6.79422021e-01\n",
      "  -1.15472162e+00  3.54342937e-01  6.34163761e+00 -7.91103601e-01\n",
      "  -2.10647392e+00 -1.50071418e+00 -1.91657102e+00 -2.68705249e-01\n",
      "  -2.26770473e+00 -1.30200064e+00 -1.06691504e+00  2.72816610e+00\n",
      "   1.72613943e+00 -2.44652486e+00 -8.29781651e-01 -3.34778428e-01\n",
      "   1.62032592e+00  2.79307222e+00  3.33606386e+00  7.49923050e-01\n",
      "   8.78082156e-01  1.09210384e+00 -5.49077809e-01  1.13655758e+00\n",
      "  -2.31284428e+00  2.90457821e+00  1.28407907e+00  6.35002971e-01\n",
      "   1.73733115e+00 -5.16952276e-01  5.64250052e-01  3.45349550e-01\n",
      "   4.30488014e+00 -1.57516491e+00 -3.11911774e+00 -3.44352865e+00\n",
      "  -6.81432605e-01 -1.36930537e+00  4.75300312e+00  4.26278305e+00\n",
      "   2.68363070e+00  5.79063356e-01  1.68708360e+00  3.88211131e+00\n",
      "   3.55729318e+00  1.15285528e+00  2.11371258e-01  2.24250108e-01\n",
      "   5.85404672e-02 -1.68168855e+00  2.29193139e+00  6.35577857e-01\n",
      "  -9.49284017e-01  2.67359519e+00  6.90081537e-01 -1.36967647e+00\n",
      "   1.60586047e+00  1.66571215e-02  2.46763539e+00 -3.36648560e+00\n",
      "  -1.36194670e+00 -2.37233138e+00 -1.60809374e+00  5.39324570e+00\n",
      "  -2.36295676e+00  2.00971031e+00  1.18532073e+00  3.55876827e+00\n",
      "  -1.73767924e+00  5.52243352e-01 -8.93150568e-01 -5.89871359e+00\n",
      "   3.34802222e+00  7.13672638e-01 -2.35375476e+00  2.77155936e-01\n",
      "   1.95922542e+00  3.69357657e+00 -3.68804002e+00 -3.17888832e+00\n",
      "   1.85305142e+00  6.93224192e-01  8.21635723e-01  1.98558724e+00\n",
      "  -1.09593618e+00 -2.06029844e+00  1.58599842e+00  1.98410183e-01\n",
      "   2.85610795e+00 -3.22312403e+00 -1.24029815e-01  1.34127474e+00\n",
      "   1.69501615e+00 -2.64979601e+00  2.76448631e+00 -3.15198278e+00\n",
      "   1.64017832e+00 -1.44698226e+00 -5.81179798e-01 -2.80554271e+00\n",
      "  -1.14251077e+00  3.67546678e+00  1.82384789e+00 -7.82150805e-01\n",
      "  -1.71862555e+00 -2.20267153e+00  1.15974367e+00  3.40847880e-01\n",
      "  -3.49781781e-01  1.37790239e+00 -1.74556947e+00  8.61084521e-01\n",
      "   6.75130939e+00  6.56701028e-01 -5.15932262e-01  9.75524545e-01\n",
      "  -1.60807943e+00  2.54459620e+00 -2.12930727e+00 -2.92811036e+00\n",
      "   8.10565889e-01 -1.93368745e+00  2.18443632e+00  1.60176408e+00\n",
      "   3.14592624e+00  1.63902962e+00  3.22101331e+00  9.80454504e-01\n",
      "   4.51220542e-01 -7.08327770e-01  3.00324488e+00  1.45941034e-01\n",
      "   3.25145888e+00 -2.35541368e+00  1.51882756e+00  3.96144915e+00\n",
      "   1.29731685e-01 -1.41095626e+00  8.29731584e-01  4.91659790e-01\n",
      "   1.25281858e+00  1.23305821e+00  3.44164938e-01  1.28082108e+00\n",
      "   4.16534042e+00  5.76524258e-01  2.41265678e+00  6.59536660e-01\n",
      "   3.37434828e-01 -6.32027745e-01  1.04185200e+00 -3.62659812e+00\n",
      "  -3.82046151e+00 -6.50587499e-01  4.08247292e-01  1.00079417e+00\n",
      "   1.30955243e+00  1.18191338e+00 -1.14697242e+00  2.06008577e+00\n",
      "   3.46009135e+00 -9.10761833e-01  3.24121022e+00  2.23682141e+00\n",
      "  -3.76572943e+00  7.93267488e-01 -2.44158340e+00  2.65548515e+00\n",
      "  -3.13371563e+00 -2.31555796e+00  1.21929002e+00  4.46840785e-02\n",
      "   1.03175318e+00 -2.50595069e+00 -4.03587818e-01  1.89385879e+00\n",
      "   1.76918101e+00  1.15773416e+00 -1.98140681e+00  1.87574673e+00\n",
      "  -1.23907375e+00 -3.00970793e+00 -3.39665383e-01  2.83806348e+00\n",
      "   4.74746257e-01 -1.94402492e+00  8.50238860e-01  2.33362108e-01\n",
      "   2.60424447e+00 -1.11007166e+00  1.12212503e+00 -1.63612866e+00\n",
      "  -2.19928741e+00  1.85618258e+00  1.91308513e-01 -4.35199052e-01\n",
      "  -1.81938601e+00  4.11620665e+00  3.11045313e+00 -3.20599198e+00\n",
      "   1.58352399e+00 -1.15508534e-01  1.69692314e+00 -3.25552285e-01\n",
      "   2.03314018e+00  2.62064409e+00  1.68929911e+00 -9.55019474e-01\n",
      "   3.98014045e+00 -6.12661982e+00  4.28673893e-01 -1.12926853e+00\n",
      "   4.71709698e-01  7.29178190e-01 -1.13756275e+00  1.51383662e+00\n",
      "   1.37846780e+00  6.19562685e-01 -4.95770264e+00 -1.00254667e+00\n",
      "   8.67521405e-01  3.94728827e+00 -2.78793782e-01  3.31308627e+00\n",
      "   6.29304838e+00 -1.23678994e+00  1.18401659e+00 -4.36786741e-01\n",
      "  -1.34622407e+00 -4.23776722e+00 -7.84476221e-01 -2.67632246e+00\n",
      "   4.11597586e+00  1.42195570e+00  2.26782262e-01  1.04578733e+00\n",
      "  -1.52226937e+00  1.09946477e+00  4.75560904e-01  3.92673445e+00\n",
      "  -1.79725885e+00 -1.43890154e+00 -3.29620647e+00  1.59642112e+00\n",
      "  -6.28980219e-01  3.29264379e+00  3.32154822e+00 -3.84628743e-01\n",
      "   9.18180227e-01  1.38396537e+00 -2.92472214e-01 -2.40428686e+00\n",
      "   7.53954029e+00  3.17816567e+00 -1.61541569e+00  1.60815847e+00\n",
      "  -3.53494096e+00  2.13561988e+00 -4.38431883e+00  1.82020403e-02\n",
      "   9.67893422e-01 -1.87881947e+00  2.46721482e+00  3.52636051e+00\n",
      "   4.50749427e-01  3.42928171e+00  2.54115534e+00  7.81728551e-02\n",
      "  -1.18559949e-01 -1.36379254e+00  1.38679135e+00  3.54284930e+00\n",
      "   2.94270897e+00  2.29453850e+00  8.10100460e+00 -1.02143955e+00\n",
      "  -5.65867424e-02  6.06740618e+00  1.64497662e+00 -3.37851834e+00\n",
      "   2.02950835e+00  1.09306395e-01  1.17275253e-01 -1.14652324e+00\n",
      "   2.89301491e+00  4.99326324e+00  6.94258451e-01  8.76925290e-01\n",
      "   5.22177756e-01  2.60136867e+00 -2.46704742e-01  4.81838793e-01\n",
      "   1.68310905e+00  1.87797987e+00  2.32121754e+00 -5.10868728e-01\n",
      "   2.01528266e-01  6.87075332e-02 -3.60551000e-01  1.68058348e+00\n",
      "  -7.72698581e-01  4.13338393e-01  2.05772662e+00 -2.55450869e+00\n",
      "  -1.12328315e+00 -2.09802842e+00  4.27623367e+00  1.03866136e+00\n",
      "   1.09092689e+00 -1.19176865e+00 -9.74748015e-01 -3.83862317e-01\n",
      "  -5.54099500e-01  3.52633524e+00  2.96513343e+00  1.23816562e+00\n",
      "   1.34755456e+00  1.07534993e+00  5.05777836e+00 -7.01843679e-01\n",
      "   1.22598398e+00  4.79537916e+00 -1.84913874e+00  1.34737110e+00\n",
      "  -1.44948423e+00 -1.59448147e+00 -2.01534438e+00 -1.33072174e+00\n",
      "   4.70535278e-01  1.61088598e+00  1.16709912e+00 -1.72390473e+00\n",
      "   3.63337326e+00 -5.00877202e-01  9.56833124e-01  5.36439991e+00\n",
      "  -3.04575634e+00  1.68803200e-01 -1.34267700e+00 -1.60667384e+00\n",
      "   2.74523079e-01 -7.43915558e-01  1.91443670e+00 -2.10682869e+00\n",
      "  -6.17630386e+00 -2.88249516e+00 -9.61544693e-01  2.00892735e+00\n",
      "  -1.36579439e-01  4.56673875e-02 -2.36677432e+00  5.65644860e-01\n",
      "  -3.19159955e-01 -3.47172570e+00  2.50497651e+00  6.48600245e+00\n",
      "   1.31990540e+00 -6.66853309e-01  8.52106571e-01  1.16769338e+00\n",
      "   4.96055424e-01 -5.14701530e-02  2.48101830e+00 -3.32218862e+00\n",
      "   3.49190903e+00  2.88168025e+00  1.82280874e+00  1.88632834e+00\n",
      "   6.39961839e-01 -6.09977841e-01  4.46560621e+00 -1.52113700e+00\n",
      "  -1.30651510e+00  8.28929722e-01 -3.39091718e-01  4.38320875e+00\n",
      "   3.01905227e+00  2.27427512e-01  6.85166597e-01  4.19887924e+00\n",
      "  -1.41773009e+00  1.74644434e+00  2.79159999e+00  5.31837463e-01\n",
      "   2.79501128e+00  4.06956291e+00 -5.76905906e-01  2.43783474e+00\n",
      "  -3.74470067e+00  5.33587098e-01 -7.52562702e-01 -2.51162100e+00\n",
      "   1.05944610e+00  8.65139186e-01  2.99773550e+00 -1.36781251e+00\n",
      "   1.09811032e+00 -2.34849310e+00 -4.01987457e+00 -1.05865848e+00\n",
      "   5.13817739e+00 -3.95154190e+00  7.18796909e-01  4.78469282e-02\n",
      "   1.41934109e+00  1.49201477e+00  1.28978074e+00  2.86678290e+00\n",
      "  -2.84953147e-01  5.26696301e+00 -3.02560258e+00  1.00212729e+00\n",
      "  -4.00508356e+00  8.76272917e-01  1.02125919e+00  1.58937395e+00\n",
      "   2.93579483e+00  7.03231215e-01  2.27966380e+00 -6.64699197e-01\n",
      "   4.27665424e+00  4.18447542e+00  5.99772632e-01  2.05835176e+00\n",
      "  -2.26305246e+00  2.02066243e-01  1.35144901e+00  7.35300601e-01\n",
      "   5.53269768e+00  4.47981834e+00  1.25907347e-01  1.71599060e-01\n",
      "  -4.64045143e+00  1.13550448e+00  1.12035894e+00  2.19001102e+00\n",
      "   6.30405724e-01 -2.63251400e+00 -6.80096149e-01 -2.66996574e+00\n",
      "   1.73724711e+00  3.12558579e+00  3.69589520e+00 -5.36866486e-01\n",
      "  -1.50925100e+00  2.50889421e+00  1.25771368e+00  7.68083930e-01\n",
      "  -1.80039549e+00 -9.76986468e-01 -1.92400411e-01  7.46473968e-01\n",
      "  -2.93732786e+00  4.52395767e-01  3.85326654e-01 -2.24435687e+00\n",
      "  -1.50503933e+00 -5.06187773e+00 -1.96101499e+00 -3.65754390e+00\n",
      "  -4.21907991e-01 -1.73763168e+00 -2.03622913e+00  7.79467404e-01\n",
      "   6.61450505e-01  5.10965049e-01  7.14508653e-01 -7.68207192e-01\n",
      "  -2.49968916e-01 -9.75140631e-01 -1.00446844e+00 -2.03578568e+00\n",
      "   8.74275208e-01 -1.79378235e+00 -1.30443871e+00 -1.73335183e+00\n",
      "   4.06898379e-01  6.02583811e-02  2.45515034e-02 -8.11237931e-01\n",
      "  -8.08502793e-01 -1.30081034e+00  1.71341765e+00 -7.32965708e-01\n",
      "  -1.72214901e+00  7.92860985e-01 -2.66530681e+00 -1.30012357e+00\n",
      "   5.30270696e-01 -9.57528651e-02  9.89459902e-02 -2.62236428e+00\n",
      "   2.18076396e+00  3.03869891e+00 -4.63765591e-01 -1.25218976e+00\n",
      "   3.25970471e-01 -1.25760663e+00 -1.44790781e+00  4.77373213e-01\n",
      "  -9.71189618e-01 -4.78351623e-01  1.67176700e+00 -3.21902573e-01\n",
      "  -3.90783119e+00 -8.99742603e-01 -1.37467921e+00 -3.74248266e+00\n",
      "  -1.35668528e+00 -1.78144026e+00 -2.24239302e+00  1.89289629e-01\n",
      "   3.93474013e-01 -3.09484386e+00 -1.17526937e+00 -3.83973265e+00\n",
      "  -4.29776573e+00 -2.21848083e+00 -2.73312712e+00  1.21375597e+00\n",
      "   5.10810375e-01 -4.60666084e+00  1.68077040e+00  3.81929946e+00]]\n",
      "tabby 0.6951178\n",
      "Egyptian cat 0.19320494\n",
      "tiger cat 0.10432123\n",
      "lynx 0.0023393033\n",
      "tiger 0.0018135043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3026231/3268163529.py:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  image = image.resize((width, height), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "session_fp32 = onnxruntime.InferenceSession(\"/home/xlx/code/neural-compressor/examples/pytorch/image_recognition/torchvision_models/quantization/qat/fx/saved_results/resnet18_float.onnx\",providers=['CUDAExecutionProvider'])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def run_sample(session, image_file, categories):\n",
    "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
    "    print(output)\n",
    "    output = output.flatten()\n",
    "    output = softmax(output) # this is optional\n",
    "    # print(output)\n",
    "    top5_catid = np.argsort(-output)[:5]\n",
    "    for catid in top5_catid:\n",
    "        print(categories[catid], output[catid])\n",
    "\n",
    "run_sample(session_fp32, '/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/cat.jpg', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 17:40:46.988765459 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-06-29 17:40:46.988801939 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m catid \u001b[39min\u001b[39;00m top5_catid:\n\u001b[1;32m     14\u001b[0m         \u001b[39mprint\u001b[39m(categories[catid], output[catid])\n\u001b[0;32m---> 16\u001b[0m run_sample(session_int8, \u001b[39m'\u001b[39;49m\u001b[39m/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/cat.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m, categories)\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mrun_sample\u001b[0;34m(session, image_file, categories)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_sample\u001b[39m(session, image_file, categories):\n\u001b[0;32m----> 9\u001b[0m     output \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mrun([], {\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m:preprocess_image(image_file, image_height, image_width)})[\u001b[39m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     11\u001b[0m     output \u001b[39m=\u001b[39m softmax(output) \u001b[39m# this is optional\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_height' is not defined"
     ]
    }
   ],
   "source": [
    "session_int8 = onnxruntime.InferenceSession(\"/home/xlx/code/neural-compressor/examples/pytorch/image_recognition/torchvision_models/quantization/qat/fx/saved_results/resnet18int.onnx\",providers=['CUDAExecutionProvider'])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def run_sample(session, image_file, categories):\n",
    "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
    "    output = output.flatten()\n",
    "    output = softmax(output) # this is optional\n",
    "    top5_catid = np.argsort(-output)[:5]\n",
    "    for catid in top5_catid:\n",
    "        print(categories[catid], output[catid])\n",
    "\n",
    "run_sample(session_int8, '/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/cat.jpg', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "def prepare_data_loaders(data_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    dataset = torchvision.datasets.ImageNet(\n",
    "        data_path, split=\"train\", transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset_test = torchvision.datasets.ImageNet(\n",
    "        data_path, split=\"val\", transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "data_path = '/data1/data/imagenet2012/'\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 1\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_inputs = (next(iter(data_loader_test))[0]) # get an example input\n",
    "# print(example_inputs.shape)\n",
    "# for step, (batch_x, batch_y) in enumerate(data_loader_test):\n",
    "#             # training\n",
    "#         print(\"steop:{}, batch_x:{}, batch_y:{}\".format(step, batch_x.numpy().shape, batch_y.shape))\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod\n",
    "class ImagenetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_loader):\n",
    "        self.data = data_loader\n",
    "\n",
    "    def get_next(self):\n",
    "        \n",
    "        self.enum_data_dicts = iter([{'input': batch_x.numpy()} for step, (batch_x, batch_y) in enumerate(self.data)])\n",
    "        return next(self.enum_data_dicts, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Quantize the model with ONNXRuntime \n",
    "In this step, we load the full precison model, and quantize it with ONNXRuntime quantization tool. And show the model size comparison between full precision and quantized model. Finally, we run the same sample with the quantized model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement a CalibrationDataReader\n",
    "CalibrationDataReader takes in calibration data and generates input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod\n",
    "import os\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "def preprocess_func(images_folder, height, width, size_limit=6000):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    unconcatenated_batch_data = []\n",
    "    num=0\n",
    "    for image_name in batch_filenames:\n",
    "        image_filepath = images_folder + '/' + image_name\n",
    "        # print(image_filepath)\n",
    "        image = Image.open(image_filepath)\n",
    "        if len(image.split())==3:\n",
    "            # print(image_filepath)\n",
    "            num=num+1\n",
    "             \n",
    "        # print(len(image.split()))\n",
    "        if len(image.split())==1 or len(image.split())==2 or len(image.split())==4:\n",
    "            continue\n",
    "        image_data = preprocess_image(image_filepath, height, width)\n",
    "        unconcatenated_batch_data.append(image_data)\n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    # print(\"claibration_data num:\",num)\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class MobilenetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder):\n",
    "        self.image_folder = calibration_image_folder\n",
    "        self.preprocess_flag = True\n",
    "        self.enum_data_dicts = []\n",
    "        self.datasize = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.preprocess_flag:\n",
    "            self.preprocess_flag = False\n",
    "            nhwc_data_list = preprocess_func(self.image_folder, image_height, image_width, size_limit=5000)\n",
    "            self.datasize = len(nhwc_data_list)\n",
    "            # print(nhwc_data_list[0].shape)\n",
    "            self.enum_data_dicts = iter([{'input': nhwc_data} for nhwc_data in nhwc_data_list])\n",
    "        return next(self.enum_data_dicts, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_folder = \"/data1/data/imagenet2012/dataILSVRC2012_img_val\"\n",
    "dr = MobilenetDataReader(calibration_data_folder)\n",
    "# dr_torch=ImagenetDataReader(data_loader_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "/tmp/ipykernel_25947/3268163529.py:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  image = image.resize((width, height), Image.ANTIALIAS)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# change it to your real calibration data set\n",
    "\n",
    "quantize_static('/home/xlx/code/quantize/resnet50_float.onnx',\n",
    "                '/home/xlx/code/quantize/resnet50_int8.onnx',\n",
    "                dr,\n",
    "                quant_format= QuantFormat.QOperator, # 量化格式 QDQ / QOperator\n",
    "                activation_type=QuantType.QInt8, # 激活类型 Int8 / UInt8\n",
    "                weight_type=QuantType.QInt8, # 参数类型 Int8 / UInt8\n",
    "                calibrate_method=CalibrationMethod.Percentile,  # 数据校准方法 MinMax / Entropy / Percentile\n",
    "                 \n",
    "                extra_options={\"ActivationSymmetric\":True,\"WeightSymmetric\":True,\"extra.Sigmoid.nnapi\":True},\n",
    "            \n",
    "\n",
    "\n",
    "                )\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"/home/xlx/code/quantize/resnet50_float.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"/home/xlx/code/quantize/resnet50_int8_percentile.onnx\")/(1024*1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m catid \u001b[39min\u001b[39;00m top5_catid:\n\u001b[1;32m     15\u001b[0m         \u001b[39mprint\u001b[39m(categories[catid], output[catid])\n\u001b[0;32m---> 17\u001b[0m run_sample(session_fp32, \u001b[39m'\u001b[39m\u001b[39m/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/cat.jpg\u001b[39m\u001b[39m'\u001b[39m, categories)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "session_fp32 = onnxruntime.InferenceSession(\"/home/xlx/code/quantize/resnet50_uint8.onnx\",providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def run_sample(session, image_file, categories):\n",
    "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
    "    output = output.flatten()\n",
    "    output = softmax(output) # this is optional\n",
    "    print(output)\n",
    "    top5_catid = np.argsort(-output)[:5]\n",
    "    for catid in top5_catid:\n",
    "        print(categories[catid], output[catid])\n",
    "\n",
    "run_sample(session_fp32, '/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/cat.jpg', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 224, 224)\n",
      "ONNX full precision model size (MB): 44.582664489746094\n",
      "ONNX quantized model size (MB): 11.220354080200195\n",
      "float32测试\n",
      "5.87ms\n",
      "4.96ms\n",
      "4.80ms\n",
      "4.76ms\n",
      "4.70ms\n",
      "4.74ms\n",
      "4.80ms\n",
      "4.78ms\n",
      "4.75ms\n",
      "4.72ms\n",
      "Avg: 4.89ms\n",
      "int8测试\n",
      "5.92ms\n",
      "5.39ms\n",
      "5.27ms\n",
      "5.25ms\n",
      "5.24ms\n",
      "5.26ms\n",
      "5.24ms\n",
      "5.24ms\n",
      "5.25ms\n",
      "5.23ms\n",
      "Avg: 5.33ms\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import onnxruntime as rt\n",
    "import torch\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "input_data = np.random.random([1,3,224,224])\n",
    "print(input_data.shape)\n",
    "\n",
    "onnx_bin=\"/home/xlx/code/quantize/resnet.bin\"\n",
    "input_data.astype(np.float32).tofile(onnx_bin)\n",
    "def get_cosine_dist(x, y):\n",
    "    cosine_dist = spatial.distance.cosine(x.reshape(-1), y.reshape(-1))\n",
    "    return cosine_dist\n",
    "def bench_performance(model_path):\n",
    "    \"\"\"\n",
    "    用于测试速度\n",
    "    :param model_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    session = onnxruntime.InferenceSession(model_path,providers=['CUDAExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    total = 0.0\n",
    "    runs = 10\n",
    "    input_data = np.zeros((1,3,224,224), np.float32)  # 随便输入一个假数据，注意shape要与模型一致，我这里是灰度图输入所以(1,1)，三通道图为(1,3)\n",
    "    # warming up\n",
    "    _ = session.run([], {input_name: input_data})\n",
    "    for i in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = session.run([], {input_name: input_data})\n",
    "        end = (time.perf_counter() - start) * 1000\n",
    "        total += end\n",
    "        print(f\"{end:.2f}ms\")\n",
    "    total /= runs\n",
    "    print(f\"Avg: {total:.2f}ms\")\n",
    "\n",
    "def bench_accuracy(model_path,onnx_bin):\n",
    "    \"\"\"\n",
    "    用于测试精度\n",
    "    :param model_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    session = rt.InferenceSession(model_path,providers=['CUDAExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    \n",
    "    onnx_input= np.fromfile(onnx_bin,dtype=np.float32).reshape(1, 3, 224, 224)\n",
    "   \n",
    "    # warming up\n",
    "    _ = session.run([], {input_name: onnx_input})\n",
    "\n",
    "    return _[0]\n",
    "\n",
    "\n",
    "input_model_path = '/home/xlx/code/neural-compressor/examples/pytorch/image_recognition/torchvision_models/quantization/qat/fx/saved_results/resnet18_float.onnx'  # 输入onnx模型\n",
    "output_model_path = '/home/xlx/code/neural-compressor/examples/pytorch/image_recognition/torchvision_models/quantization/qat/fx/saved_results/resnet18int.onnx'  # 输出模型名\n",
    "\n",
    "\n",
    "# dist = get_cosine_dist(bench_accuracy(input_model_path,onnx_bin),bench_accuracy(output_model_path,onnx_bin))\n",
    "# print(dist)\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(input_model_path)/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(output_model_path)/(1024*1024))\n",
    "print(\"float32测试\")\n",
    "bench_performance(input_model_path)\n",
    "print(\"int8测试\")\n",
    "bench_performance(output_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_height = 512\n",
    "image_width = 512\n",
    "# change it to your real calibration data set\n",
    "calibration_data_folder = \"calibration_imagenet\"\n",
    "dr = MobilenetDataReader(calibration_data_folder)\n",
    "print(dr)\n",
    "quantize_static('/data1/need_quant_models/deeplabv3plus_r101-d8_4xb2-80k_cityscapes-512x1024.onnx',\n",
    "                '/data1/need_quant_models/deeplabv3plus_r101-d8_4xb2-80k_cityscapes-512x1024_uint8.onnx',\n",
    "                dr,\n",
    "                quant_format= QuantFormat.QOperator, # 量化格式 QDQ / QOperator\n",
    "                per_channel=True,\n",
    "                activation_type=QuantType.QUInt8, # 激活类型 Int8 / UInt8\n",
    "                weight_type=QuantType.QInt8, # 参数类型 Int8 / UInt8\n",
    "                calibrate_method=CalibrationMethod.MinMax, ) # 数据校准方法 MinMax / Entropy / Percentile\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"/data1/need_quant_models/deeplabv3plus_r101-d8_4xb2-80k_cityscapes-512x1024.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"/data1/need_quant_models/deeplabv3plus_r101-d8_4xb2-80k_cityscapes-512x1024_uint8.onnx\")/(1024*1024))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it to your real calibration data set\n",
    "calibration_data_folder = \"calibration_imagenet\"\n",
    "dr = MobilenetDataReader(calibration_data_folder)\n",
    "\n",
    "quantize_static('/home/xlx/code/quantization/swinv2_tiny_window8_256.ms_in1k.onnx',\n",
    "                '/home/xlx/code/quantization/swinv2_tiny_window8_256.ms_in1k_uint8.onnx',\n",
    "                dr)\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"/home/xlx/code/quantization/swinv2_tiny_window8_256.ms_in1k.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"/home/xlx/code/quantization/swinv2_tiny_window8_256.ms_in1k_uint8.onnx\")/(1024*1024))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can not upload full calibration data set for copy right issue, we only demonstrate with some example images. You need to use your own calibration data set in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model_in_file=\"/home/xlx/code/onnxruntime-inference-examples/quantization/notebooks/imagenet_v2/mobilenet_v2_float.onnx\"\n",
    "model=onnx.load(model_in_file)\n",
    "nodes = model.graph.node\n",
    "for node in nodes:\n",
    "    print(node.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Run the model with OnnxRuntime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
